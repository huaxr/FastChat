import torch
import torch.nn as nn

# 要训练一个随机初始化的 torch.nn.Linear，需要进行以下步骤：
#
# 定义训练数据集和标签。
# 定义损失函数，通常使用交叉熵或均方误差。
# 定义优化器，通常使用随机梯度下降（SGD）或自适应优化器（例如Adam）。
# 重复以下步骤，直到达到指定的训练轮数或收敛到一个较好的模型：
# 将训练数据传递给模型，获得模型的输出。
# 计算损失函数。
# 计算损失函数的梯度。
# 使用优化器更新模型的权重。
# 以下是一个简单的示例代码，展示如何训练一个随机初始化的 torch.nn.Linear：

# 在这个示例中，我们生成一个大小为（100，10）的随机输入张量，大小为（100，1）的随机标签张量。
# 我们使用 nn.Linear 定义一个单层神经网络，将其输入大小设置为10，输出大小设置为1。
# 我们定义均方误差（nn.MSELoss）作为损失函数，并使用随机梯度下降（torch.optim.SGD）作为优化器。
# 我们重复100次迭代，并在每次迭代中计算损失和更新权重。

# 定义数据集和标签
X = torch.randn(100, 10)
y = torch.randn(100, 1)

# 定义模型 在 PyTorch 中，模型可以看作是由一个或多个层（layer）组成的序列或者图，
# 每一层都是一个函数，可以接受输入，并计算输出。nn.Linear 本身就是一个 PyTorch 中的层（layer），
# 它内部包含了一个张量参数，可以根据输入自动计算输出，并且可以反向传播优化这个参数，因此可以被看作是一个模型。
# 在实际使用时，我们可以将多个层组合起来形成一个更复杂的模型。
model = nn.Linear(10, 1)

# 定义损失函数和优化器
criterion = nn.MSELoss()  # 均方误差

# 这行代码定义了一个 SGD (stochastic gradient descent) 优化器，用于优化模型参数。
# SGD 是一种基本的优化算法，其思想是在每一步迭代中，计算一个 batch 的样本的损失函数梯度，
# 然后以一定的步长（学习率 lr）沿着梯度反方向更新模型参数。这个过程会不断重复，直到损失函数收敛或达到一定的迭代次数。
# model.parameters() 返回模型的所有参数，包括线性层的权重和偏置。这些参数可以通过优化器进行更新。
# 在这个例子中，我们使用了 SGD 优化器，学习率为 0.01。
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

if __name__ == "__main__":
    # 训练模型
    for epoch in range(10000000):
        # 前向传递
        y_pred = model(X)

        # 计算损失函数
        # 在机器学习中，模型的训练是通过最小化损失函数来实现的。在训练过程中，模型预测的输出与实际的标签之间会存在一定的误差，
        # 这个误差就是损失函数。损失函数是一种度量模型误差的函数，它将模型的预测输出和真实标签作为输入，输出一个标量值，表示模型的误差大小。
        # 在代码中，criterion是损失函数，y_pred是模型的预测输出，y是真实标签，计算损失的过程就是将y_pred和y作为输入传入criterion
        # 中，计算得到一个标量值作为模型的误差大小。
        loss = criterion(y_pred, y)

        # 反向传播并更新权重
        optimizer.zero_grad()
        # 是用来清空所有参数的梯度，这是为了避免在反向传播时累加梯度。在每次迭代中，我们需要在计算梯度之前调用zero_grad()。
        optimizer.zero_grad()

        # loss.backward()是PyTorch
        # 中用于计算损失函数相对于模型参数的梯度的函数。在反向传播时，PyTorch会自动计算各个参数的梯度，并保存在相应的参数张量的.grad
        # 属性中。这样我们就可以使用这些梯度更新模型参数，以使得损失函数的值更小。
        loss.backward()

        # optimizer.step()
        # 是优化器中的一个方法，用于根据梯度更新模型中的参数。具体地，它会根据之前调用backward()
        # 方法计算得到的梯度，以及在初始化优化器时传入的学习率和其他参数，计算出参数的更新量，并更新模型中的参数。
        # 在使用PyTorch训练模型时，一般会先调用zero_grad()
        # 清空之前的梯度，再调用backward()
        # 计算梯度，最后调用step()
        # 更新模型参数。这个过程可以理解为每一次迭代，先计算梯度，再更新参数，以使模型的损失函数在训练过程中逐渐下降。
        optimizer.step()

        # 打印损失函数
        print(f"Epoch {epoch + 1}, Loss: {loss.item():.4f}")
